# -*- coding: utf-8 -*-
"""Week6_Dist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WmcVt5YyRaxLT3imO21mvHpNcJL5ypBg
"""

def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    # Number of data points: n
    n = len(data)

    # x-data for the ECDF: x
    x = np.sort(data)

    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n

    return x, y

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_excel('Week6_imp.xlsx')
df.head()

df1=df[['M6','T6','W6','TH6','F6']]

df1.mean(),df1.std()

df1.isnull().sum()

(df1[['M6']]>=5).value_counts(),(df1[['T6']]>=5).value_counts(),(df1[['W6']]>=5).value_counts(),(df1[['TH6']]>=5).value_counts(),(df1[['F6']]>=5).value_counts()

243-179

757+64

64/821

"""###BERNOULLI DISTRIBUTION"""

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.33,0.66]
ax.bar(langs,students)
plt.title('Monday')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.5,0.5]
ax.bar(langs,students)
plt.title('Tuesday')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.64,0.35]
ax.bar(langs,students)
plt.title('Wednesday')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.8,0.2]
ax.bar(langs,students)
plt.title('Thursday')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.92,0.07]
ax.bar(langs,students)
plt.title('Friday')
plt.show()

df2=df1
df2=df2.fillna(0)
df2

df2.to_excel('Week6_befimp.xlsx')

(df1[['M6']]>=0).value_counts(),(df1[['T6']]>=0).value_counts(),(df1[['W6']]>=0).value_counts(),(df1[['TH6']]>=0).value_counts(),(df1[['F6']]>=0).value_counts()

821/1000

"""###BINOMIAL DISTRIBUTION"""

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Present','Absent']
students = [0.82,0.18]
ax.bar(langs,students)
plt.title('Monday')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Present','Absent']
students = [0.82,0.18]
ax.bar(langs,students)
plt.title('Tuesday')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Present','Absent']
students = [0.82,0.18]
ax.bar(langs,students)
plt.title('Wednesday')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Present','Absent']
students = [0.78,0.22]
ax.bar(langs,students)
plt.title('Thursday')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Present','Absent']
students = [0.82,0.18]
ax.bar(langs,students)
plt.title('Friday')
plt.show()

import seaborn as sns
sns.distplot(df1['M6'],kde=False)

"""###POISSON DISTRIBUTION FOR MOMENTS OF DATA"""

for i in df1.columns:
    plt.figure()
    sns.distplot(df1[i],kde=True)

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imp_mice = IterativeImputer(n_nearest_features=5)
df_mice=imp_mice.fit_transform(df1)
df_mice=pd.DataFrame(df_mice,columns=df1.columns)

fig = plt.figure()
ax = fig.add_subplot(111)
df['M6'].plot(kind='kde', ax=ax, color='blue')
df_mice['M6'].plot(kind='kde', ax=ax, color='pink',title='MICE')

(df_mice[['M6']]>=5).value_counts(),(df_mice[['T6']]>=5).value_counts(),(df_mice[['W6']]>=5).value_counts(),(df_mice[['TH6']]>=5).value_counts(),(df_mice[['F6']]>=5).value_counts()

"""###BERNOULLI DISTRIBUTION"""

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.31,0.69]
ax.bar(langs,students)
plt.title('Monday_mice')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.48,0.52]
ax.bar(langs,students)
plt.title('Tuesday_mice')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.6,0.4]
ax.bar(langs,students)
plt.title('Wednesday_mice')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.78,0.22]
ax.bar(langs,students)
plt.title('Thursday_mice')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.91,0.09]
ax.bar(langs,students)
plt.title('Friday_mice')
plt.show()

df_mice=df_mice.round(0)

"""###POISSON DISTRIBUTION"""

for i in df_mice:
    plt.figure()
    sns.distplot(df_mice[i],kde=True)

df_Monday=[df['M1'],df['M2'],df['M3'],df['M4'],df['M5'],df_mice['M6']]
df_Monday=pd.DataFrame(df_Monday)
df_Monday=df_Monday.T
df_Monday

from sklearn.decomposition import PCA

# Feature matrix and target array
X = df_Monday

# PCA
pca = PCA(n_components=2)

# Fit and transform
principalComponents = pca.fit_transform(X)

# Print ratio of variance explained
print(pca.explained_variance_ratio_)

Monday=pd.DataFrame(principalComponents)
Monday

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# For loop
for n_clusters in range(2,10):
    kmeans = KMeans(n_clusters=n_clusters)
    # Fit and predict your k-Means object
    preds = kmeans.fit_predict(Monday)
    score = silhouette_score(Monday, preds, metric='euclidean')
    print ("For n_clusters = {}, silhouette score is {})".format(n_clusters, score))

sum_of_squared_distances = []

# Create for loop
for k in range(1,15):
    kmeans = KMeans(n_clusters=k)
    kmeans = kmeans.fit(Monday)
    sum_of_squared_distances.append(kmeans.inertia_)

# Plot
plt.plot(range(1,15), sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum of squared distances')
plt.title('Elbow Method For Optimal k')
plt.show()

kmeans_monday=KMeans(n_clusters=3, random_state=123)
kmeans_monday.fit(Monday)
pred_monday=kmeans_monday.predict(Monday)

df_mice['cluster_Monday']= pred_monday

df_Tuesday=[df['T1'],df['T2'],df['T3'],df['T4'],df['T5'],df_mice['T6']]
df_Tuesday=pd.DataFrame(df_Tuesday)
df_Tuesday=df_Tuesday.T
df_Tuesday

from sklearn.decomposition import PCA

# Feature matrix and target array
X = df_Tuesday

# PCA
pca = PCA(n_components=2)

# Fit and transform
principalComponents = pca.fit_transform(X)

# Print ratio of variance explained
print(pca.explained_variance_ratio_)

Tuesday=pd.DataFrame(principalComponents)
Tuesday

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# For loop
for n_clusters in range(2,10):
    kmeans = KMeans(n_clusters=n_clusters)
    # Fit and predict your k-Means object
    preds = kmeans.fit_predict(Tuesday)
    score = silhouette_score(Tuesday, preds, metric='euclidean')
    print ("For n_clusters = {}, silhouette score is {})".format(n_clusters, score))

sum_of_squared_distances = []

# Create for loop
for k in range(1,15):
    kmeans = KMeans(n_clusters=k)
    kmeans = kmeans.fit(Tuesday)
    sum_of_squared_distances.append(kmeans.inertia_)

# Plot
plt.plot(range(1,15), sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum of squared distances')
plt.title('Elbow Method For Optimal k')
plt.show()

kmeans_tuesday=KMeans(n_clusters=4, random_state=123)
kmeans_tuesday.fit(Tuesday)
pred_tuesday=kmeans_tuesday.predict(Tuesday)

df_mice['cluster_Tuesday']= pred_tuesday

df_Wednesday=[df['W1'],df['W2'],df['W3'],df['W4'],df['W5'],df_mice['W6']]
df_Wednesday=pd.DataFrame(df_Wednesday)
df_Wednesday=df_Wednesday.T
df_Wednesday

from sklearn.decomposition import PCA

# Feature matrix and target array
X = df_Wednesday

# PCA
pca = PCA(n_components=2)

# Fit and transform
principalComponents = pca.fit_transform(X)

# Print ratio of variance explained
print(pca.explained_variance_ratio_)

Wednesday=pd.DataFrame(principalComponents)
Wednesday

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# For loop
for n_clusters in range(2,10):
    kmeans = KMeans(n_clusters=n_clusters)
    # Fit and predict your k-Means object
    preds = kmeans.fit_predict(Wednesday)
    score = silhouette_score(Wednesday, preds, metric='euclidean')
    print ("For n_clusters = {}, silhouette score is {})".format(n_clusters, score))

sum_of_squared_distances = []

# Create for loop
for k in range(1,15):
    kmeans = KMeans(n_clusters=k)
    kmeans = kmeans.fit(Wednesday)
    sum_of_squared_distances.append(kmeans.inertia_)

# Plot
plt.plot(range(1,15), sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum of squared distances')
plt.title('Elbow Method For Optimal k')
plt.show()

kmeans_wednesday=KMeans(n_clusters=4, random_state=123)
kmeans_wednesday.fit(Wednesday)
pred_wednesday=kmeans_wednesday.predict(Wednesday)

df_mice['cluster_Wednesday']= pred_wednesday

df_Thursday=[df['TH1'],df['TH2'],df['TH3'],df['TH4'],df['TH5'],df_mice['TH6']]
df_Thursday=pd.DataFrame(df_Thursday)
df_Thursday=df_Thursday.T
df_Thursday

from sklearn.decomposition import PCA

# Feature matrix and target array
X = df_Thursday

# PCA
pca = PCA(n_components=2)

# Fit and transform
principalComponents = pca.fit_transform(X)

# Print ratio of variance explained
print(pca.explained_variance_ratio_)

Thursday=pd.DataFrame(principalComponents)
Thursday

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# For loop
for n_clusters in range(2,10):
    kmeans = KMeans(n_clusters=n_clusters)
    # Fit and predict your k-Means object
    preds = kmeans.fit_predict(Thursday)
    score = silhouette_score(Thursday, preds, metric='euclidean')
    print ("For n_clusters = {}, silhouette score is {})".format(n_clusters, score))

sum_of_squared_distances = []

# Create for loop
for k in range(1,15):
    kmeans = KMeans(n_clusters=k)
    kmeans = kmeans.fit(Thursday)
    sum_of_squared_distances.append(kmeans.inertia_)

# Plot
plt.plot(range(1,15), sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum of squared distances')
plt.title('Elbow Method For Optimal k')
plt.show()

kmeans_thursday=KMeans(n_clusters=2, random_state=123)
kmeans_thursday.fit(Thursday)
pred_thursday=kmeans_thursday.predict(Thursday)

df_mice['cluster_Thursday']= pred_thursday

df_Friday=[df['F1'],df['F2'],df['F3'],df['F4'],df['F5'],df_mice['F6']]
df_Friday=pd.DataFrame(df_Friday)
df_Friday=df_Friday.T
df_Friday

from sklearn.decomposition import PCA

# Feature matrix and target array
X = df_Friday

# PCA
pca = PCA(n_components=2)

# Fit and transform
principalComponents = pca.fit_transform(X)

# Print ratio of variance explained
print(pca.explained_variance_ratio_)

Friday=pd.DataFrame(principalComponents)
Friday

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# For loop
for n_clusters in range(2,10):
    kmeans = KMeans(n_clusters=n_clusters)
    # Fit and predict your k-Means object
    preds = kmeans.fit_predict(Friday)
    score = silhouette_score(Friday, preds, metric='euclidean')
    print ("For n_clusters = {}, silhouette score is {})".format(n_clusters, score))

sum_of_squared_distances = []

# Create for loop
for k in range(1,15):
    kmeans = KMeans(n_clusters=k)
    kmeans = kmeans.fit(Friday)
    sum_of_squared_distances.append(kmeans.inertia_)

# Plot
plt.plot(range(1,15), sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum of squared distances')
plt.title('Elbow Method For Optimal k')
plt.show()

kmeans_friday=KMeans(n_clusters=2, random_state=123)
kmeans_friday.fit(Friday)
pred_friday=kmeans_friday.predict(Friday)

df_mice['cluster_Friday']= pred_friday

df_mice

import seaborn as sns
sns.set_theme(style="whitegrid")
sns.boxplot(x="cluster_Monday", y="M6", data=df_mice)

sns.boxplot(x="cluster_Tuesday", y="T6", data=df_mice)

sns.boxplot(x="cluster_Wednesday", y="W6", data=df_mice)

sns.boxplot(x="cluster_Thursday", y="TH6", data=df_mice)

sns.boxplot(x="cluster_Friday", y="F6", data=df_mice)

from sklearn.ensemble import RandomForestRegressor

clf_m = RandomForestRegressor()
clf_t=RandomForestRegressor()
clf_w=RandomForestRegressor()
clf_th=RandomForestRegressor()
clf_f=RandomForestRegressor()

clf_m.fit(df_mice[['T6','cluster_Monday']],df_mice[['M6']])

from sklearn.metrics import r2_score
r2_score(df_mice[['M6']],clf_m.predict(df_mice[['T6','cluster_Monday']]))

clf_t.fit(df_mice[['M6','W6','cluster_Tuesday']],df_mice[['T6']])

from sklearn.metrics import r2_score
r2_score(df_mice[['T6']],clf_t.predict(df_mice[['M6','W6','cluster_Tuesday']]))

clf_w.fit(df_mice[['T6','TH6','cluster_Wednesday']],df_mice[['W6']])

from sklearn.metrics import r2_score
r2_score(df_mice[['W6']],clf_w.predict(df_mice[['T6','TH6','cluster_Wednesday']]))

clf_th.fit(df_mice[['W6','F6','cluster_Thursday']],df_mice[['TH6']])

from sklearn.metrics import r2_score
r2_score(df_mice[['TH6']],clf_th.predict(df_mice[['W6','F6','cluster_Thursday']]))

clf_f.fit(df_mice[['TH6','cluster_Friday']],df_mice[['F6']])

from sklearn.metrics import r2_score
r2_score(df_mice[['F6']],clf_f.predict(df_mice[['TH6','cluster_Friday']]))

M=clf_m.predict(df_mice[['T6','cluster_Monday']])
T=clf_t.predict(df_mice[['M6','W6','cluster_Tuesday']])
W=clf_w.predict(df_mice[['T6','TH6','cluster_Wednesday']])
TH=clf_th.predict(df_mice[['W6','F6','cluster_Thursday']])
F=clf_f.predict(df_mice[['TH6','cluster_Friday']])

a=[M,T,W,TH,F]
b=pd.DataFrame(a)
b_final=b.round(0)
c=b_final.T
c

(c[[0]]>=5).value_counts(),(c[[1]]>=5).value_counts(),(c[[2]]>=5).value_counts(),(c[[3]]>=5).value_counts(),(c[[4]]>=5).value_counts()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.29,0.71]
ax.bar(langs,students)
plt.title('Monday_pred')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.48,0.52]
ax.bar(langs,students)
plt.title('Tuesday_pred')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.63,0.37]
ax.bar(langs,students)
plt.title('Wednesday_pred')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.78,0.22]
ax.bar(langs,students)
plt.title('Thursday_pred')
plt.show()

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['Pass','Fail']
students = [0.93,0.07]
ax.bar(langs,students)
plt.title('Friday_pred')
plt.show()

for i in c:
    plt.figure()
    sns.distplot(c[i],kde=True)

df1.to_excel('Week6_miss.xlsx')
df_mice.to_excel('Week6_mice.xlsx')
c.to_excel('Week6_pred.xlsx')

df_Friday=[df['F1'],df['F2'],df['F3'],df['F4'],df['F5']]
df_f1=np.array(df_Friday)
df_f1.mean(),df_f1.std()

from scipy.stats import norm

Normal_Friday=norm.rvs(size=1000,loc=6.07,scale=1.2)
sns.distplot(Normal_Friday,kde=True)
plt.title('Friday')

df_Friday=[df['F1'],df['F2'],df['F3'],df['F4'],df['F5'],df_mice['F6']]
df_f2=np.array(df_Friday)
df_f2.mean(),df_f2.std()

Normal_Friday=norm.rvs(size=1000,loc=6.16,scale=1.27)
sns.distplot(Normal_Friday,kde=True)
plt.title('Friday_mice')

df_Friday=[df['F1'],df['F2'],df['F3'],df['F4'],df['F5'],df_mice['F6'],c[4]]
df_f=np.array(df_Friday)
df_f.mean(),df_f.std()

Normal_Friday=norm.rvs(size=1000,loc=6.23,scale=1.32)
sns.distplot(Normal_Friday,kde=True)
plt.title('Friday_pred')

df_Monday=[df['M1'],df['M2'],df['M3'],df['M4'],df['M5']]
df_m1=np.array(df_Monday)
df_m1.mean(),df_m1.std()

Normal_Monday=norm.rvs(size=1000,loc=3.2,scale=1.5)
sns.distplot(Normal_Monday,kde=True)
plt.title('Monday')

df_Monday=[df['M1'],df['M2'],df['M3'],df['M4'],df['M5'],df_mice['M6']]
df_m2=np.array(df_Monday)
df_m2.mean(),df_m2.std()

Normal_Monday=norm.rvs(size=1000,loc=3.26,scale=1.6)
sns.distplot(Normal_Monday,kde=True)
plt.title('Monday_mice')

df_Monday=[df['M1'],df['M2'],df['M3'],df['M4'],df['M5'],df_mice['M6'],c[0]]
df_m=np.array(df_Monday)
df_m.mean(),df_m.std()

Normal_Monday=norm.rvs(size=1000,loc=3.3,scale=1.6)
sns.distplot(Normal_Monday,kde=True)
plt.title('Monday_pred')

df_Tuesday=[df['T1'],df['T2'],df['T3'],df['T4'],df['T5']]
df_t1=np.array(df_Tuesday)
df_t1.mean(),df_t1.std()

Normal_Tuesday=norm.rvs(size=1000,loc=4,scale=1.5)
sns.distplot(Normal_Tuesday,kde=True)
plt.title('Tuesday')

df_Tuesday=[df['T1'],df['T2'],df['T3'],df['T4'],df['T5'],df_mice['T6']]
df_t2=np.array(df_Tuesday)
df_t2.mean(),df_t2.std()

Normal_Tuesday=norm.rvs(size=1000,loc=4,scale=1.5)
sns.distplot(Normal_Tuesday,kde=True)
plt.title('Tuesday_mice')

df_Tuesday=[df['T1'],df['T2'],df['T3'],df['T4'],df['T5'],df_mice['T6'],c[1]]
df_t=np.array(df_Tuesday)
df_t.mean(),df_t.std()

Normal_Tuesday=norm.rvs(size=1000,loc=4.05,scale=1.5)
sns.distplot(Normal_Tuesday,kde=True)
plt.title('Tuesday_pred')

df_Wednesday=[df['W1'],df['W2'],df['W3'],df['W4'],df['W5']]
df_w1=np.array(df_Wednesday)
df_w1.mean(),df_w1.std()

Normal_Wednesday=norm.rvs(size=1000,loc=4.7,scale=1.4)
sns.distplot(Normal_Wednesday,kde=True)
plt.title('Wednesday')

df_Wednesday=[df['W1'],df['W2'],df['W3'],df['W4'],df['W5'],df_mice['W6']]
df_w2=np.array(df_Wednesday)
df_w2.mean(),df_w2.std()

Normal_Wednesday=norm.rvs(size=1000,loc=4.7,scale=1.4)
sns.distplot(Normal_Wednesday,kde=True)
plt.title('Wednesday_mice')

df_Wednesday=[df['W1'],df['W2'],df['W3'],df['W4'],df['W5'],df_mice['W6'],c[2]]
df_w=np.array(df_Wednesday)
df_w.mean(),df_w.std()

Normal_Wednesday=norm.rvs(size=1000,loc=4.75,scale=1.38)
sns.distplot(Normal_Wednesday,kde=True)
plt.title('Wednesday_pred')

df_Thursday=[df['TH1'],df['TH2'],df['TH3'],df['TH4'],df['TH5']]
df_TH1=np.array(df_Thursday)
df_TH1.mean(),df_TH1.std()

Normal_Thursday=norm.rvs(size=1000,loc=5.4,scale=1.3)
sns.distplot(Normal_Thursday,kde=True)
plt.title('Thursday')

df_Thursday=[df['TH1'],df['TH2'],df['TH3'],df['TH4'],df['TH5'],df_mice['TH6']]
df_TH2=np.array(df_Thursday)
df_TH2.mean(),df_TH2.std()

Normal_Thursday=norm.rvs(size=1000,loc=5.48,scale=1.3)
sns.distplot(Normal_Thursday,kde=True)
plt.title('Thursday_mice')

df_Thursday=[df['TH1'],df['TH2'],df['TH3'],df['TH4'],df['TH5'],df_mice['TH6'],c[3]]
df_TH=np.array(df_Thursday)
df_TH.mean(),df_TH.std()

Normal_Thursday=norm.rvs(size=1000,loc=5.5,scale=1.3)
sns.distplot(Normal_Thursday,kde=True)
plt.title('Thursday_pred')

df1.mean(),df1.std()

df_mice.mean(),df_mice.std()

c.mean(),c.std()

df1.median(),df1.mode()

df_mice.median(),df_mice.mode()

c.median(),c.mode()


